{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the score for a Youtube Video generated text file for Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Loading the required modules\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize,RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For representing the 0s or negetive reviews as sad face and 1s or positive reviews as happy face\n",
    "emoji_dict = {\"0\" : \":crying_face:\",\n",
    "              \"1\" : \":grinning_face_with_big_eyes:\"}\n",
    "\n",
    "def print_emoji(label):\n",
    "    \n",
    "    return emoji.emojize(emoji_dict[label], use_aliases=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 64)                640064    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 640,129\n",
      "Trainable params: 640,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = load_model('best_model.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_idx = imdb.get_word_index()   ## The dictionary for word to its index from the source dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = dict([value, key] for (key, value) in word_idx.items())  ## Reversing this dict to get index to word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Our predefined sentence-vectorizing function\n",
    "\n",
    "def vect_sent(sentences, dims=10000):\n",
    "    \n",
    "    outputs = np.zeros((len(sentences),dims))\n",
    "    \n",
    "    for i,idx in enumerate(sentences):\n",
    "        outputs[i,idx] = 1\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(snt):\n",
    "\n",
    "    sent = snt.split()\n",
    "    test = []\n",
    "    \n",
    "    for ix in sent:\n",
    "        try:\n",
    "            if word_idx[ix]+3 > 10000:      ## the first 3 letters are .?/ so we need to skip them\n",
    "                test.append(9999)           ## if the word index exceeds the 10000 vocabuary, set it to last index\n",
    "            else:\n",
    "                test.append(word_idx[ix]+3)\n",
    "        \n",
    "        except:\n",
    "                test.append(9999)\n",
    "    \n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = [\"it is really bad and poor quality\", \"was really easy to hold\"]\n",
    "\n",
    "tes = []\n",
    "for s in sent:\n",
    "    \n",
    "    tes.append(func(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tes = vect_sent(tes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ðŸ˜¢\n",
      "1 ðŸ˜ƒ\n"
     ]
    }
   ],
   "source": [
    "## Predicting the vectors on the pretrained model weights :: 0==>bad review, 1==>good review\n",
    "\n",
    "pred = model.predict_classes(tes)\n",
    "\n",
    "for ix in range(len(pred)):\n",
    "    print(ix, print_emoji(str(ix)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The text file contains copied data from the Youtube transcript of 3 reviews on Nikon D5600 Camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "data = glob.glob(\"youtube-api extracted transcripts/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = set(stopwords.words('english'))    ## Stopwords like a the so it etc that have no specific meaning are removed\n",
    "tokenizer = RegexpTokenizer(\"[a-zA-Z@]+\")   ## RegexTokenizer removes unwanted text like D5600 which has no meaning\n",
    "\n",
    "scores = []\n",
    "\n",
    "for vid_file in data:\n",
    "    \n",
    "    try:\n",
    "        with open(vid_file, 'r') as in_file:   #Loading the text file adn splitting it in sentences\n",
    "            text = in_file.read()\n",
    "            sents = sent_tokenize(text)\n",
    "\n",
    "        data = []\n",
    "        data = sents[0].split('\\n')\n",
    "        reviews = []\n",
    "\n",
    "        for ix in range(0,len(data),2):     # Every 2nd sentence is a timestamp which we need to skip\n",
    "            reviews.append(data[ix])\n",
    "\n",
    "        for ix in range(len(reviews)):\n",
    "\n",
    "            word_list = tokenizer.tokenize(reviews[ix].lower())\n",
    "            useful_words = [w for w in word_list if w not in sw and int]\n",
    "            reviews[ix] = ' '.join(useful_words)\n",
    "\n",
    "        reviews = np.array(reviews)\n",
    "\n",
    "        tes = []\n",
    "        for s in reviews:\n",
    "            tes.append(func(s))\n",
    "\n",
    "        tes = vect_sent(tes)\n",
    "        pred = model.predict_classes(tes)\n",
    "\n",
    "        score = np.sum(pred==[1])/pred.shape[0]\n",
    "        scores.append(score)\n",
    "    \n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7366435830908725\n"
     ]
    }
   ],
   "source": [
    "scores = np.array(scores)\n",
    "\n",
    "print(np.average(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prediction on the given Video scraped data is that 73.7% of the sentences used in youtube are good reviews\n",
    "## i.e it was a positive review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.         0.7625     0.64102564 0.6826087  1.         0.67213115\n",
      " 0.47777778 0.51351351 0.5952381  0.65454545 0.75       0.57894737\n",
      " 0.72222222 0.74774775 0.65625    1.         0.9        0.69886364\n",
      " 0.73469388 0.75471698 0.88235294 0.74846626 1.         0.58181818\n",
      " 0.88888889 0.72340426 0.5        0.60869565 0.66666667 1.\n",
      " 0.75163399 0.67213115 0.63461538 0.65124555 1.         0.69411765\n",
      " 0.69565217 1.         0.85882353 0.67857143 0.64705882 0.5879397\n",
      " 1.         0.70535714 0.75       0.62222222 0.63636364 0.75862069\n",
      " 0.68656716 0.57894737 0.66666667 0.72727273 0.51111111 0.78571429\n",
      " 0.60606061 0.42857143 0.75555556 0.75       1.         1.\n",
      " 0.615      0.7027027  1.         0.69230769 0.75581395 1.\n",
      " 0.67973856 1.         0.625      0.73214286 0.66666667 1.\n",
      " 0.66935484 0.57407407 0.82352941 0.68131868 0.57627119 0.79310345\n",
      " 0.5978836  0.78571429 0.70103093 1.         0.64772727 0.63120567\n",
      " 0.79569892 1.         0.65740741 0.62903226 0.7        0.70967742\n",
      " 0.73333333 0.84848485 0.55789474 0.59602649 0.68867925 0.71584699\n",
      " 0.7480315  0.67567568 0.63888889 0.65641026 0.82608696 0.64444444\n",
      " 1.         0.53703704 0.71153846 0.625      0.63333333 0.66964286\n",
      " 0.57627119 0.63448276 0.75409836 0.71698113 0.59124088 0.86046512\n",
      " 0.81395349 0.74285714 0.85       0.92307692 1.         0.69230769\n",
      " 0.725      1.         0.7        0.77777778 0.69354839 1.\n",
      " 0.66666667 0.68571429 0.65384615 0.70422535 0.59580052 0.75757576\n",
      " 0.65640194 0.69642857 1.         0.73684211 0.75       0.78571429\n",
      " 0.61658456 1.         0.92307692 0.73109244 1.         0.71578947\n",
      " 0.92682927 0.59235669 0.72413793 0.69767442 0.80952381 0.74375\n",
      " 0.91666667 0.6        0.63522013 1.         0.7826087  1.\n",
      " 0.69148936 0.56716418 0.62931034 1.         0.9375     0.72222222\n",
      " 0.575      0.64102564 0.64788732 0.         0.54545455 0.59375\n",
      " 0.55813953 0.60098522 1.         0.81481481 0.725      1.\n",
      " 0.57777778 0.73251029]\n"
     ]
    }
   ],
   "source": [
    "## the reviews can be checked here::\n",
    "\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
